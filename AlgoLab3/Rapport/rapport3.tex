\documentclass[10pt,a4paper]{article}
\usepackage{geometry}
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern} \normalfont
\DeclareFontShape{T1}{lmr}{bx}{sc}{<-> ssub * cmr/bx/sc}{}
\usepackage{textcomp}
\usepackage{datetime}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{tocloft}
\usepackage{fixltx2e}
\usepackage{color}
\usepackage{algpseudocode}
\usepackage[colorlinks=true,
			linkcolor=blue,
			bookmarksnumbered=true,
			pdftitle={Rapport INF4705},
			pdfauthor={Eric Ah-Tiane, Gwenegan Hudin},
			pdfborder={0 0 0},
			pdfsubject={Rapport TP3 Algorithmique}]{hyperref}

% Custom commands
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\Section}[1]{\section*{#1} \addcontentsline{toc}{section}{#1} \setcounter{subsection}{0}}
%\renewcommand*{\theHsection}{chY.\the\value{section}}
\renewcommand{\thesection}{\Roman{section}.}
\renewcommand{\thesubsection}{\arabic{subsection}.}
\renewcommand{\thesubsubsection}{\alph{subsubsection}.}
\renewcommand{\cftsecnumwidth}{2em}
\renewcommand{\cftsubsecnumwidth}{2em}
\renewcommand{\cftsubsubsecnumwidth}{2em}
\addto\captionsfrench{
	\renewcommand{\cfttoctitlefont}{\Large}
	\renewcommand{\contentsname}{\centering \textsc{Table des Matières}\\[0.5cm]}
}

\renewcommand{\baselinestretch}{1.15}

\begin{document}

\begin{titlepage}
	\begin{center}
		\begin{figure}
        \begin{subfigure}[c]{0.2\textwidth}
        		\centering
                \includegraphics[width=0.6\textwidth]{images/logo-polymtl}
        \end{subfigure}
		\end{figure}
		
		
		\vspace{30pt}
		\textsc{\huge Génie Informatique}\\
		\textsc{\LARGE Rapport de Travaux Pratiques}\\		
		\vfill
		
		% Title
		\HRule \\[0.7cm]
		{\Huge \bfseries INF4705 : Lab 3}\\[0.4cm]
		{\Large Résolution de problème combinatoire}\\[0.2cm]
		\HRule\\[1cm]
		
		\vfill
		
		% Author
		\begin{minipage}{0.49\textwidth}
			\begin{flushleft} \LARGE
				\textbf{Auteur}\\
				Eric \textsc{Ah-Tiane}\\ 1760376\\
				Gwenegan \textsc{Hudin}\\ 1756642\\[0.5cm]
			\end{flushleft}
		\end{minipage}
		\begin{minipage}{0.49\textwidth}
			\begin{flushright} \LARGE
				\textbf{Rendu}\\
				5 Décembre 2014\\ À Polytechnique Montréal\\[0.5cm]
			\end{flushright}
		\end{minipage}
	\end{center}
\end{titlepage}

\newpage

\hfill

\newpage

\tableofcontents

\newpage

\section{Introduction}

Le présent rapport documente les procédures utilisées lors du troisième laboratoire du cours INF4705 Analyse et Conception d'algorithmes.

Le laboratoire consiste à implémenter un algorithme d’optimisation afin de résoudre un problème combinatoire. Il faut minimiser les pertes liées à l’affectation des coupes dans les blocs de marbre. Nous avons donc un immense espace de solutions potentielles à explorer, parmi lesquelles de solutions optimales locales et une solution optimale globale.

Le choix de l’algorithme était laissé au choix mais il a été décidé d’utiliser un algorithme vorace randomisé avec amélioration locale. Cette méthode permet de déterminer rapidement plusieurs solutions optimales locales. Parmi celles-ci, il est possible, au bout d'un nombre d'essais assez grand, de trouver l'optimum global de l'espace de solutions.

Les sections suivantes présentes l’algorithme implanté et son analyse ainsi que l’originalité de celui-ci. Cet algorithme sera en compétition avec ceux des autres groupes du cours, et se verra attribuer seulement 3 minutes pour trouver la meilleure solution possible, après quoi son fonctionnement sera interrompu. Le temps d'exécution est donc important.

Des ensembles de données nous sont fournies avec 2, 3, 4, 5 ou 9 catégories de blocs.

\newpage

\section{Présentation de l'algorithme}

\subsection{Fonctionnement et pseudo-code}

Lors de notre remue-méninges initial, nous avons considéré l'utilisation de la programmation par contraintes, particulièrement adaptée pour résoudre les problèmes combinatoires. Nous voulions utiliser le cadriciel Choco avec Java.
Cependant, la programmation par contraintes sort de l'objectif de ce cours, et nous nous sommes donc rabattus sur la solution préconisée avec les outils à notre disposition. Nous avons implémenté en Java un algorithme vorace randomisé, avec une heuristique d'amélioration locale, et répétons ces opérations pour trouver différents optimums locaux. De ces optimums, nous gardons le meilleur, qui peut être l'optimum global (mais ce n'est pas garanti). Voir Figure 2.

\begin{figure}[h!]
\begin{algorithmic}

\Function{ResoudreProbleme}{P}
	\State $s_{opt}\gets \{\}$
	\State $p \gets \infty$
	
	\While{true}
		\State $s \gets $ \Call{voraceRandomise}{P, $\alpha$}
		\State $s \gets $ \Call{ameliorationLocale}{$s$}
		\If{\Call{Perte}{$s_{opt}$} < \Call{Perte}{$s$} }
			\State $s_{opt}\gets s$
			\State \Call{afficher}{s}
		\EndIf
	\EndWhile
\EndFunction

\end{algorithmic}
\caption{Pseudo-code de l'algorithme global}
\end{figure}

Un vorace non randomisé obtiendrait toujours la même solution, et amènerait donc toujours au même optimum local, qui n'est probablement pas l'optimum global du premier coup.
On introduit donc un certain degré d'aléatoire en ne prenant non pas toujours la meilleure solution disponible, mais une des meilleures solutions disponibles, avec un intervalle généré par $ \alpha $. On obtient ainsi une liste réduite de candidats parmi lesquels on choisit aléatoirement. Voir Figure 3.

\newpage

\begin{figure}[h!]
\begin{algorithmic}
\Function{voraceRandomise}{P, $\alpha$}
	\State $s \gets \{\}$
	\State $couts \gets \{\}$
	\State $coupes \gets $ \Call{initialiserCandidats}{P}
	
	\ForAll{$c \in coupes$}
		\State $couts[c] \gets $ \Call{evaluer}{c}
	\EndFor
	
	\ForAll{$c \in coupes$}
		\State $min \gets $ \Call{minimum}{couts}
		\State $max \gets $ \Call{maximum}{couts}
		\State $RCL \gets \{\}$
		\State $borne \gets max - \alpha \times (max - min) $
		\State $RCL \gets \{c \in coupes \mid cout[coupe] \leq borne\} $
		\State $e^{*} \gets $ \Call{random}{RCL}
		\State \Call{ajouter}{$e^{*}$, s}
		\State \Call{miseAJour}{coupes}
		\State \Call{miseAJour}{couts}
	\EndFor	
	\State \Return s
\EndFunction

\end{algorithmic}
\caption{Pseudo-code de la méthode vorace randomisée}
\end{figure}

Une fois que l'on a une solution vorace, on l'améliore localement par la méthode de descente de pente 1-opt. Cet algorithme intervertit une assignation de coupe aléatoirement au sein de la solution, et si elle est meilleure que la solution originale, alors il la garde et essaie à nouveau.
Cet algorithme s'arrête lorsqu'il échoue un certain nombre de fois à la suite, ce qui indique qu'il n'a probablement plus aucun échange intéressant à réaliser, et donc qu'il a atteint l'optimum local. Ce nombre a été fixé à 30 empiriquement dans notre programme. Voir Figure 4.

\newpage

\begin{figure}[h!]
\begin{algorithmic}

\Function{ameliorationLocale}{$s_{0}$}
	\State $s_{opt} \gets \{\}$
	
	\While{$essais < essais_{max}$}	
		\State $ bloc1 \gets $ \Call{random}{$s_{0}$}
		\State $ coupe \gets $ \Call{random}{bloc1[coupes]}
		\State $ recepteurs \gets \{bloc \in s_{0} \mid perte[bloc] \geq coupe\}$
		\State $ bloc2 \gets $ \Call{random}{$recepteurs$}  \Comment{$bloc2 \neq bloc1$}
		\State \Call{transfererCoupe}{bloc1, bloc2}
		\State $s_{opt} \gets $ \Call{remplacer}{$s_{0}$, bloc1, bloc2}
		\State \Call{reduire}{$s_{opt}$}
		\If{erreur ou \Call{vide}{recepteurs} ou \Call{Perte}{$s_{opt}$} > \Call{Perte}{$s_{0}$}}
			\State essais++
		\Else
			\State $essais \gets 0$
			\State $s_{0} \gets s_{opt}$
		\EndIf
	\EndWhile
	\State \Return $s_{opt}$
\EndFunction

\end{algorithmic}
\caption{Pseudo-code de la méthode d'amélioration locale 1-opt}
\end{figure}

\subsection{Analyse de complexité}

Soient $n$ le nombre de coupes disponibles et $c$ le nombre de capacités disponibles.
La complexité de la fonction de calcul vorace est déterminée par la complexité de la fonction de réduction de la solution $reduire()$.
Comme sa complexité est de $\theta(n \times c)$, la complexité de calcul vorace est aussi de $\theta(n \times c)$.

La complexité de la fonction d'amélioration locale est plus difficile à déterminer. En effet, sa complexité dépend de la taille de l'ensemble des solutions voisines.
Pour calculer une solution voisine, la complexité est à nouveau déterminée par $reduire()$. Donc la complexité pour calculer une solution voisine est de $\theta(n \times c)$.
Toutefois, le nombre de solutions voisines possibles est inconnu. L'algorithme d'amélioration locale possède donc une complexité dépendant de la taille de l'ensemble des solutions voisines et de $\theta(n \times c)$.

\subsection{Originalité}

Notre méthode est classique et ne présente pas d'originalité particulière. Notons que la recherche de voisinage dans l'algorithme d'amélioration locale force la descente le long de la pente, et ne monte jamais. Cela coupe une partie de l'espace de solution qu'il pourrait sembler intéressant d'explorer. Cependant, nous avons essayé les deux méthodes, et forcer la descente permettait d'obtenir de meilleurs résultats bien plus rapidement.

Nous avons aussi exploré les possibilités d'utiliser une amélioration 2-opt, en échangeant à chaque fois 2 coupes au lieu d'une. Là aussi, les résultats étaient moins probants, et le temps d'exécution augmentait beaucoup trop (3 fois plus de temps pour trouver une solution équivalente au 1-opt). Il a donc été abandonné.

\section{Conclusion}

L'algorithme a subi de nombreuses corrections itératives lorsque nous avons pu utiliser le vérificateur de solutions, que nous avons confrontés à nos solutions sur les exemples 2\_0, 2\_9, 4\_8, 5\_3, 9\_1 et 9\_9.

Nos solutions, à l'heure du rendu, ont toutes été validées par ce programme, et nous pensons donc avoir couvert toutes les possibilités.

Que le meilleur gagne !

\end{document}
